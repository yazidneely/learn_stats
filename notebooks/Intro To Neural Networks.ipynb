{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31150337-2396-45fc-93bc-0cb81911c0b0",
   "metadata": {},
   "source": [
    "# Intro to Neural Networks\n",
    "Building a simple neural network in Python is a great way to understand the fundamentals of deep learning. We'll use the NumPy library for numerical operations, which keeps things straightforward without relying on higher-level frameworks initially.\n",
    "Here, we'll create a basic feedforward neural network with one hidden layer to solve a simple classification problem (e.g., the XOR problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76170733-fdc3-4ecc-a41c-ae41cf277ea9",
   "metadata": {},
   "source": [
    "## Conceptual Overview\n",
    "1. **Input Layer:** Receives the data.\n",
    "2. **Hidden Layer(s):** Performs computations and learns patterns.\n",
    "3. **Output Layer:** Produces the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a0b9d-91fd-4d6a-b4e6-7a118a216399",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "- **Weights and Biases:** Parameters that the network learns during training. Weights determine the strength of the connection between neurons, and biases shift the activation function.\n",
    "- **Activation Function:** Introduces non-linearity into the network, allowing it to learn complex relationships. Common ones include Sigmoid, ReLU, and Tanh. For this example, we'll use Sigmoid.\n",
    "- **Forward Propagation:** The process of passing input data through the network to get an output.\n",
    "- **Loss Function:** Measures how well the network's predictions match the actual values (e.g., Mean Squared Error).\n",
    "- **Backpropagation:** The core algorithm for training neural networks. It calculates the gradients of the loss function with respect to the weights and biases, indicating how much to adjust them.\n",
    "- **Gradient Descent:** An optimization algorithm that uses the gradients to iteratively update the weights and biases to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1918c-3133-4182-8c76-d76cf7edc6c6",
   "metadata": {},
   "source": [
    "# Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b7e982-489e-4b81-a318-4877e7ea978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8eb6e8-4967-49a1-bb6d-e6fb5b5cc200",
   "metadata": {},
   "source": [
    "## 1. Define the Sigmoid Activation Function and its Derivative \n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "$$\\frac{d}{dx}\\sigma(x) = x(1-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7867918-a2f3-4bbb-8dcb-4434feeacbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7448d72-433f-4205-83ee-a13f00ef730d",
   "metadata": {},
   "source": [
    "## 2. Prepare the Dataset (XOR Problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5de3b5-24a2-4c31-b8b8-dcb63913a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "# Output dataset\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01895c-f4be-4f2c-a51f-d94c8a0514c8",
   "metadata": {},
   "source": [
    "## 3. Set up Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e462c596-1da9-4518-bc3f-ebbdc472eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Number of neurons in the input layer\n",
    "input_neurons = X.shape[1] # 2 features\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "hidden_neurons = 50 # You can experiment with this number\n",
    "\n",
    "# Number of neurons in the output layer\n",
    "output_neurons = y.shape[1] # 1 output\n",
    "\n",
    "# Initialize weights and biases randomly with mean 0\n",
    "# Weights for input to hidden layer\n",
    "weights_input_hidden = 2 * np.random.random((input_neurons, hidden_neurons)) - 1\n",
    "# Biases for hidden layer\n",
    "bias_hidden = np.zeros((1, hidden_neurons))\n",
    "\n",
    "# Weights for hidden to output layer\n",
    "weights_hidden_output = 2 * np.random.random((hidden_neurons, output_neurons)) - 1\n",
    "# Biases for output layer\n",
    "bias_output = np.zeros((1, output_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e784b60-215f-49c4-b417-960328d0a5f1",
   "metadata": {},
   "source": [
    "## 4. Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b726b99e-4497-4ed8-8fc8-0c7cc507b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network...\n",
      "Epoch 0, Loss: 0.5025\n",
      "Epoch 1000, Loss: 0.3922\n",
      "Epoch 2000, Loss: 0.1850\n",
      "Epoch 3000, Loss: 0.1094\n",
      "Epoch 4000, Loss: 0.0796\n",
      "Epoch 5000, Loss: 0.0639\n",
      "Epoch 6000, Loss: 0.0542\n",
      "Epoch 7000, Loss: 0.0475\n",
      "Epoch 8000, Loss: 0.0426\n",
      "Epoch 9000, Loss: 0.0388\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.0358\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000 # Number of iterations\n",
    "learning_rate = 0.1 # How much to adjust weights each time\n",
    "\n",
    "print(\"Training the Neural Network...\")\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # --- Forward Propagation ---\n",
    "    # Layer 1 (Hidden Layer)\n",
    "    # Dot product of X and weights_input_hidden + bias\n",
    "    layer1_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    # Apply sigmoid activation\n",
    "    layer1_output = sigmoid(layer1_input)\n",
    "\n",
    "    # Layer 2 (Output Layer)\n",
    "    # Dot product of layer1_output and weights_hidden_output + bias\n",
    "    layer2_input = np.dot(layer1_output, weights_hidden_output) + bias_output\n",
    "    # Apply sigmoid activation (our prediction)\n",
    "    predicted_output = sigmoid(layer2_input)\n",
    "\n",
    "    # --- Backpropagation ---\n",
    "    # Calculate the error for the output layer\n",
    "    # Error is the difference between actual (y) and predicted\n",
    "    output_error = y - predicted_output\n",
    "    # Apply the derivative of the sigmoid to the error\n",
    "    output_delta = output_error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    # Calculate the error for the hidden layer\n",
    "    # How much did the hidden layer weights contribute to the output error?\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    # Apply the derivative of the sigmoid to the hidden layer error\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(layer1_output)\n",
    "\n",
    "    # --- Update Weights and Biases ---\n",
    "    # Adjust weights and biases based on the deltas and learning rate\n",
    "    weights_hidden_output += layer1_output.T.dot(output_delta) * learning_rate\n",
    "    bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "    bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Optional: Print loss every few epochs to monitor progress\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.abs(output_error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final Loss: {np.mean(np.abs(y - predicted_output)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b140fed-ef48-4729-8b5c-a4c0c78035bb",
   "metadata": {},
   "source": [
    "## 5. Make Predictions (Test the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6088b9a5-3edc-4544-8af1-65134150065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions after training:\n",
      "Input:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output:\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output (Raw):\n",
      " [[0.03]\n",
      " [0.96]\n",
      " [0.96]\n",
      " [0.04]]\n",
      "Predicted Output (Rounded):\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n",
      "Prediction for input [[0 1]]: 0.9645 (Rounded: 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions after training:\")\n",
    "\n",
    "# You can use the same forward propagation logic to test with new data\n",
    "def predict(X_new):\n",
    "    layer1_input_test = np.dot(X_new, weights_input_hidden) + bias_hidden\n",
    "    layer1_output_test = sigmoid(layer1_input_test)\n",
    "\n",
    "    layer2_input_test = np.dot(layer1_output_test, weights_hidden_output) + bias_output\n",
    "    predicted_output_test = sigmoid(layer2_input_test)\n",
    "    return predicted_output_test\n",
    "\n",
    "# Test with our training data to see how well it learned\n",
    "predictions = predict(X)\n",
    "print(\"Input:\\n\", X)\n",
    "print(\"Actual Output:\\n\", y)\n",
    "print(\"Predicted Output (Raw):\\n\", predictions)\n",
    "print(\"Predicted Output (Rounded):\\n\", np.round(predictions))\n",
    "\n",
    "# You can also test with a single new input\n",
    "new_input = np.array([[0, 1]])\n",
    "new_prediction = predict(new_input)\n",
    "print(f\"\\nPrediction for input {new_input}: {new_prediction[0][0]:.4f} (Rounded: {np.round(new_prediction)[0][0]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163ef3f-383a-4382-8384-8a327abda910",
   "metadata": {},
   "source": [
    "# Explanation of the Code:\n",
    "1. **sigmoid(x) and sigmoid_derivative(x):**\n",
    "- The sigmoid function squashes any input value between 0 and 1. This is useful for probabilities or when you need a smooth, differentiable activation.\n",
    "- Its derivative is crucial for backpropagation, as it tells us the \"slope\" of the activation function at a given point, which helps determine how much to adjust the weights.\n",
    "  \n",
    "2. **Dataset (X and y):**\n",
    "- X: Our input data, representing the four possible combinations for the XOR problem ([0,0], [0,1], [1,0], [1,1]).\n",
    "- y: The corresponding desired output for each input (0 for [0,0], 1 for [0,1], 1 for [1,0], 0 for [1,1]).\n",
    "  \n",
    "3. **Network Parameters:**\n",
    "- np.random.seed(1): Ensures that the random weight initialization is the same every time you run the code, making results reproducible.\n",
    "- input_neurons, hidden_neurons, output_neurons: Define the architecture of our network.\n",
    "- weights_input_hidden, bias_hidden, weights_hidden_output, bias_output: These are the learnable parameters. They are initialized randomly (weights) or to zeros (biases) to break symmetry and allow the network to learn different features. The 2 * np.random.random(...) - 1 initializes weights between -1 and 1.\n",
    "\n",
    "4. **Training Loop (for epoch in range(epochs)):**\n",
    "- **epochs:** The number of times the entire dataset is passed forward and backward through the neural network. More epochs generally lead to better learning, but too many can lead to overfitting.\n",
    "- **learning_rate:** Controls the step size during weight updates. A smaller learning rate means slower but potentially more stable learning. A larger learning rate can speed up training but might overshoot the optimal solution.\n",
    "- **Forward Propagation:**\n",
    "  - layer1_input: The weighted sum of inputs plus the bias for the hidden layer. This is Z = WX + B.\n",
    "  - layer1_output: The result of applying the sigmoid activation function to layer1_input. This is A = sigmoid(Z).\n",
    "  - The same process is repeated for the output layer.\n",
    "- **Backpropagation:**\n",
    "  - **output_error:** The difference between what the network should have predicted (y) and what it actually predicted (predicted_output). This is our primary error signal.\n",
    "  - **output_delta:** This is the error scaled by the derivative of the output layer's activation function. This tells us how much to change the output layer's weights.\n",
    "  - **hidden_error:** To calculate the error for the hidden layer, we backpropagate the output_delta through the weights_hidden_output. This tells us how much each hidden neuron contributed to the output error.\n",
    "  - **hidden_delta:** Similar to output_delta, but for the hidden layer.\n",
    "- **Update Weights and Biases:**\n",
    "  - weights_hidden_output += ...: We update the weights by adding a fraction (learning_rate) of the dot product of the previous layer's output (which is the input to these weights) and the delta for the current layer.\n",
    "  - bias_output += ...: Biases are updated by summing the delta values and multiplying by the learning rate. This effectively pushes the activation up or down.\n",
    "  - The same update logic applies to weights_input_hidden and bias_hidden.\n",
    "\n",
    "5. **Making Predictions:**\n",
    "- After training, the predict function essentially performs forward propagation using the learned weights and biases on new input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d12a36-1c77-454b-99b1-37c48c2a60a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
